# Multi-task PBR Generator Training Configuration

model:
  pretrained: true                    # Use pretrained EfficientNet encoder
  freeze_encoder_epochs: 5            # Freeze encoder for first N epochs (warmup)
  use_text_conditioning: true         # Enable CLIP text conditioning (requires transformers)

training:
  data_dir: "./data/materials"        # Directory containing basecolor/, normal/, roughness/, metallic/, height/
  resolution: 512                     # Training resolution
  batch_size: 32                      # Reduced for encoder unfreezing at epoch 6
  epochs: 100                         # Total epochs
  learning_rate: 0.0003               # Conservative LR for stability
  weight_decay: 0.01                  # AdamW weight decay
  num_workers: 8                      # More workers to saturate H200
  augment: true                       # Enable data augmentation
  validation_epochs: 5                # Validate every N epochs

  # Performance
  use_amp: true                       # Mixed precision training (~2x faster)
  compile_model: false                # Disabled - text prompts cause recompilation

  # Loss weights (tune these based on results)
  lambda_l1: 1.0                      # L1 reconstruction loss
  lambda_perceptual: 0.1              # VGG perceptual loss
  lambda_ssim: 0.5                    # SSIM structural loss
  lambda_gradient: 0.5                # Gradient/edge loss

checkpointing:
  output_dir: "./output/multitask"    # Output directory
  save_epochs: 10                     # Save checkpoint every N epochs

logging:
  use_wandb: true                     # Log to Weights & Biases
  project_name: "pbr-multitask"       # W&B project name
